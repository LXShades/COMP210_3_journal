\documentclass{scrartcl}

\usepackage[hidelinks]{hyperref}
\usepackage[none]{hyphenat}
\usepackage{setspace}
\doublespace

\usepackage{graphicx}
\usepackage{float}
\graphicspath{{images/}}

\newcommand{\source}[1]{\caption*{Source: {#1}} }
% Above code sourced from Xavi, Stack Overflow, https://tex.stackexchange.com/questions/95029/add-source-to-figure-caption @ 20/03/2018

\title{An insight into human-computer interaction research methodologies}
\subtitle{COMP210 - Interfaces and Interaction}
\date{\today}
\author{1707981}

\begin{document}
\maketitle
\pagenumbering{arabic}

%\section{Introduction}
%- Critical Critters
%	- Wall running
%	- Static bars
%	- health bars
%	- The pause menu and Team Ghost, etc
%- Genesis
%	- Adding a modifier
%	- Hard modelling
%	- Extruding etc
%	- Bugs
%- SpyroEdit
%	- Texture editing and paletting rules
%	- Moving objects, etc
%- PlayStation Assembly Tool
%- Demiurge
%	- Pause menu usefulness
%	- Mouse effectiveness
%	- Should the crosshair scale?
%- Audio project with Mango
%    - Should we use words in the interface, or will unlabelled colours do?

%\section{Look up}
%Qualitative                Quantitative
%Interviews                 Automated Data Collection
%Focus Groups -             Physiological Data
%Diaries -                  Eye Tracking
%Camera Study-              Task Analysis
%Surveys -                  A/B testing
%Heuristic Evaluation -     Bench Marking
% Cognitive Walkthroughs -   Surveys
% Ethnographic Field Study - Click Stream Analysis
% Think Aloud Protocol -     System Usability Scale (SUS)

\section{Proposal}
This journal addresses the research techniques to be used in the assessment of the user interface for the 3D modelling program \textit{Genesis}. This research of the various usability testing methodologies will inform which one could be used to test the interface's efficiency in various domains, including:

\begin{itemize}
	\item The time it takes for a user to find an item or feature of interest
	\item How quickly a user may find an unfamiliar or advanced feature
	\item Whether any confusion is encountered whilst looking for a feature
	\item Whether it is intuitive based on a 3D modeller's expectations
	\item Whether there are any significant errors encountered
	\item Whether the user is aware of how to handle such errors
\end{itemize}

Some of these elements may be easy to measure quantitatively. Others may lend themselves to qualitative questionnaires, or A/B variants to test efficiency. However, while the source code of the application is freely available, it is unlikely that the changes could be actioned during the testing phase due to a lack of resources and time.

\section{User testing methods}
\subsection{Eye tracking}
Eye tracking involves monitoring a user's eye movement to broadly determine their focus of attention. \cite{poole_eye_2006} It traditionally uses a costly specialised camera, making the method less accessible outside of lab scenarios \cite{devicecomparison}, thereby restricting the demographic range of subjects. However, recent developments \cite{lowcosttracker} \cite{ho_low_2014} have managed to achieve similar effects with low-cost portable IR cameras. Additional developments in the field of VR suggest that eye-tracking may be a widely accessible strategy in the near future.

In HCI analysis, eye tracking takes advantage of the theory that eye movement and fixation directly correlates to the user's interest. \cite{poole_eye_2006} Specialised tools can then record and visualise the user's area of focus in a heat map which provides a visualisation immediately depicting areas of interest. However, this is unlikely to be useful alone, and a more detailed recording of the path of a user's eye focus may be more substantial \cite{gazepatterns}.

\subsection{Focus groups}


\subsection{Surveys, interviews and bias}
For a designer to get specific answers to their questions about their design approaches - such as how effectively it works - they may be tempted to ask the users directly. \textbf{Interviewing} and \textbf{surveying} are two methods of data collection wherein a user is asked a specific set of questions about their experience on a topic.

This method, however, is strongly impacted by bias. Several sources of bias, such as: the demographic difference between a voluntary respondent and a non-respondent; motivational influence of the interviewer; language and cultural differences; social desirability bias (especially on sensitive topics \cite{sensitivequestions}); and even the order of questions \cite{questionnairebias}. Many of these issues are shared across surveys, interviews and computers \cite{moresurveybias} \cite{questionnairebias}.

Surveys are closely similar to interviews in terms of the specific questions that can be addressed. Fortunately, in an ironic celebration of human-computer interfacing, computer surveys are known to be more preferred by users, faster to complete, and significantly more effective at delivering fully completed responses in general \cite{computersurveys}. They do, however, suffer from many of the same biases.(cite?)

Although these methods have advantages in the collection of qualitative data, an analysis of human-computer-interaction may well benefit more from automated data collection software. This is leveraged by the fact that an HCI evaluation will already involve a computer, making it efficiently non-invasive.

\subsection{Think Aloud Protocol}
The Think Aloud Protocol is not necessarily an independent testing process. It is a protocol wherein a user, or perhaps expert, explains their thought process aloud. This provides a specific benefit of understanding a user's thought process \cite{haak_exploring_2003}. It may allow users to express any frustration that is otherwise not indicated. This enables a designer or assessor to understand the habits of the user and challenges they are likely to face.

It is believed that thought processes can occur much faster than speech \cite{e._fonteyn_description_1993}, making an exhaustive evaluation of thoughts impossible through this method alone. However, the addition of a more automated method, such as eye tracking, coupled with additional recording and observations may produce a clearer picture. This suffers from the same issue of collating data, 

\section{Method choice comparison}

\section{Conclusions}


\subsection{Server system}
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=1.0\linewidth]{Server_Side_Streamer.png}
%	\caption{Partial class diagram illustrating the potential map streaming server},
%	\label{fig:serversystem}
%\end{figure}

\bibliography{bibliography} 
\bibliographystyle{ieeetr}

\end{document}